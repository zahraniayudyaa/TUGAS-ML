{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahraniayudyaa/TUGAS-ML-DL/blob/main/ch_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 11: Training Deep Neural Networks**"
      ],
      "metadata": {
        "id": "RLrFzDIt4CPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. MASALAH VANISHING/EXPLODING GRADIENTS**\n",
        "---\n",
        "Pada jaringan saraf dalam (deep neural networks), selama proses pelatihan dengan backpropagation, gradien dari fungsi loss dapat menjadi sangat kecil (vanishing gradients) atau sangat besar (exploding gradients) saat melewati banyak lapisan. Hal ini terjadi karena perkalian berulang dari gradien melalui setiap lapisan.\n",
        "\n",
        "Penyebab utama:\n",
        "* Fungsi aktivasi yang jenuh seperti sigmoid dan tanh memiliki turunan yang mendekati 0 untuk input besar\n",
        "* Inisialisasi bobot yang tidak tepat menyebabkan varians input/output tidak stabil\n",
        "* Jaringan yang sangat dalam memperbesar efek perkalian gradien\n",
        "\n",
        "Dampak:\n",
        "* Lapisan bawah (dekat input) tidak mendapatkan update yang berarti\n",
        "* Pelatihan menjadi sangat lambat atau tidak konvergen\n",
        "* Model tidak dapat belajar pola kompleks"
      ],
      "metadata": {
        "id": "ykTmhqi44EF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demonstrasi Vanishing Gradients dengan Sigmoid"
      ],
      "metadata": {
        "id": "NxDLXypC4Qub"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9VMjNle3-Xq"
      },
      "outputs": [],
      "source": [
        "# Fungsi sigmoid dan turunannya\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# Simulasi gradien melalui 10 layer\n",
        "np.random.seed(42)\n",
        "gradient = 1.0\n",
        "for i in range(10):\n",
        "    weights = np.random.randn(100, 100) * 1.0  # Inisialisasi buruk\n",
        "    activation_input = np.random.randn(100)\n",
        "    activation_output = sigmoid(np.dot(weights, activation_input))\n",
        "    gradient = gradient * sigmoid_derivative(np.dot(weights, activation_input)).mean()\n",
        "    print(f\"Layer {i+1}: Gradient magnitude = {gradient:.10f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. TEKNIK INISIALISASI**\n",
        "---\n",
        "Inisialisasi bobot yang tepat sangat penting untuk memastikan sinyal dapat merambat dengan baik baik maju (forward) maupun mundur (backward).\n",
        "\n",
        "**Glorot/Xavier Initialization:**\n",
        "* Dirancang untuk fungsi aktivasi sigmoid/tanh\n",
        "* Mempertahankan varians input dan output yang sama\n",
        "\n",
        "Rumus:\n",
        "* Distribusi normal: σ² = 1 / fan_avg\n",
        "* Distribusi uniform: [-√(3/fan_avg), √(3/fan_avg)]\n",
        "\n",
        "**He Initialization:**\n",
        "* Dirancang untuk ReLU dan variannya\n",
        "* Memperhitungkan sifat ReLU yang membuang setengah aktivasi\n",
        "* Rumus: σ² = 2 / fan_in\n",
        "\n",
        "**LeCun Initialization:**\n",
        "* Versi awal dari Glorot initialization\n",
        "* Efektif untuk fungsi aktivasi SELU"
      ],
      "metadata": {
        "id": "vDjbsl4n4Vy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementasi Inisialisasi di Keras"
      ],
      "metadata": {
        "id": "5M6s8dn94w5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Glorot Initialization (default untuk dense layer)\n",
        "layer_glorot = keras.layers.Dense(100, activation='tanh',\n",
        "                                 kernel_initializer='glorot_normal')\n",
        "\n",
        "# He Initialization untuk ReLU\n",
        "layer_he = keras.layers.Dense(100, activation='relu',\n",
        "                             kernel_initializer='he_normal')\n",
        "\n",
        "# LeCun Initialization untuk SELU\n",
        "layer_lecun = keras.layers.Dense(100, activation='selu',\n",
        "                                kernel_initializer='lecun_normal')\n",
        "\n",
        "# Custom VarianceScaling\n",
        "custom_init = keras.initializers.VarianceScaling(\n",
        "    scale=2.0, mode='fan_avg', distribution='uniform'\n",
        ")\n",
        "layer_custom = keras.layers.Dense(100, activation='sigmoid',\n",
        "                                 kernel_initializer=custom_init)"
      ],
      "metadata": {
        "id": "1XzTTrHQ4wfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. FUNGSI AKTIVASI NON-SATURATING**\n",
        "---\n",
        "Fungsi aktivasi tradisional (sigmoid, tanh) mengalami saturasi yang menyebabkan vanishing gradients. Fungsi aktivasi modern dirancang untuk mengatasi masalah ini.\n",
        "\n",
        "Perbandingan Fungsi Aktivasi:\n",
        "### **1. ReLU (Rectified Linear Unit):**\n",
        "* f(x) = max(0, x)\n",
        "* Kelebihan: Tidak jenuh untuk x > 0, komputasi cepat\n",
        "* Masalah: Dying ReLU (neuron mati jika selalu output 0)\n",
        "\n",
        "### **2. Leaky ReLU:**\n",
        "* f(x) = max(αx, x) dengan α kecil (biasanya 0.01)\n",
        "* Mengatasi dying ReLU dengan slope kecil untuk x < 0\n",
        "* ELU (Exponential Linear Unit):\n",
        "* f(x) = x jika x ≥ 0, α(exp(x)-1) jika x < 0\n",
        "* Mean output mendekati 0, membantu konvergensi\n",
        "* Smooth di semua titik (jika α=1)\n",
        "\n",
        "### **3. SELU (Scaled ELU):**\n",
        "* ELU yang diskalakan dengan parameter λ dan α\n",
        "* Dapat menyebabkan self-normalization\n",
        "* Syarat: arsitektur sequential, input distandardisasi"
      ],
      "metadata": {
        "id": "fVqyH-Nv4z0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementasi Fungsi Aktivasi"
      ],
      "metadata": {
        "id": "btleSQ3d5L42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model dengan berbagai fungsi aktivasi\n",
        "model_activations = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "\n",
        "    # ReLU dengan He initialization\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Activation(\"relu\"),\n",
        "\n",
        "    # Leaky ReLU\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.LeakyReLU(alpha=0.2),\n",
        "\n",
        "    # SELU dengan LeCun initialization\n",
        "    keras.layers.Dense(100, activation=\"selu\",\n",
        "                      kernel_initializer=\"lecun_normal\"),\n",
        "\n",
        "    # Output layer\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "2WGC517s5OiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. BATCH NORMALIZATION**\n",
        "---\n",
        "Batch Normalization (BN) menormalkan aktivasi setiap layer untuk mengurangi internal covariate shift. Teknik ini:\n",
        "\n",
        "* Menstabilkan distribusi input ke setiap layer\n",
        "* Memungkinkan learning rate lebih besar\n",
        "* Berfungsi sebagai regularizer ringan\n",
        "* Mengurangi ketergantungan pada inisialisasi\n",
        "\n",
        "Algoritma Batch Normalization:\n",
        "* Hitung mean dan varians mini-batch\n",
        "* Normalisasi: x̂ = (x - μ) / √(σ² + ε)\n",
        "* Scale dan shift: y = γ ⊗ x̂ + β\n",
        "\n",
        "Mode Operasi:\n",
        "* Training: Gunakan statistik batch\n",
        "* Inference: Gunakan moving averages"
      ],
      "metadata": {
        "id": "6HNt_2Jd5Rh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementasi Batch Normalization"
      ],
      "metadata": {
        "id": "gS8S5xoA5idY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model dengan Batch Normalization SETELAH aktivasi\n",
        "model_bn_after = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Dense(300, activation=\"elu\",\n",
        "                      kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Dense(100, activation=\"elu\",\n",
        "                      kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Model dengan Batch Normalization SEBELUM aktivasi\n",
        "model_bn_before = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\",\n",
        "                      use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\",\n",
        "                      use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers"
      ],
      "metadata": {
        "id": "5XpM5Y325jaX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}